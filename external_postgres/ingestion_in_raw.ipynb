{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import os\n",
    "from subprocess import call\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, INT, VARCHAR, DATE, TIMESTAMP, DECIMAL\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Schemas, Tables, and Views in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing SQL Scripts Against PostgreSQL (Schemas, Tables, Views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run SQL script using shell command\n",
    "# I had to pass the env parameters explicitly  to the subprocess.call() -> (PGPASSWORD, PGUSER, PGHOST, PGPORT, PGDATABASE)\n",
    "# This avoided Jupyter Notebook asking for password. \n",
    "def run_sql_script(script_name):\n",
    "    script_path = f\"/workspace/external_postgres/{script_name}\"\n",
    "    command = f\"psql -U {user} -d {db_name} -h {host} -p {port} -f {script_path}\"\n",
    "    return call(command, shell=True, env={\n",
    "                                        'PGPASSWORD': password,\n",
    "                                        'PGUSER': user,\n",
    "                                        'PGHOST': host,\n",
    "                                        'PGPORT': port,\n",
    "                                        'PGDATABASE': db_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Schemas Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check schema existence\n",
    "def check_schema_existence(connection_uri, schema_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Schemas exist in the database ---\")\n",
    "            for schema_name in schema_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if schema_exists:\n",
    "                    print(f\"Schema '{schema_name}' exists in the database.\")\n",
    "                else:\n",
    "                    print(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(\"----- End of Schema Checking -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Tables Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check table existence\n",
    "def check_table_existence(connection_uri, schema_name, table_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Tables exist ---\")\n",
    "            for table_name in table_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = :schema AND table_name = :table\"),\n",
    "                    {\"schema\": schema_name, \"table\": table_name}\n",
    "                )\n",
    "                table_exists = result.fetchone() is not None\n",
    "                if table_exists:\n",
    "                    print(f\"Table '{table_name}' exists in schema '{schema_name}'.\")\n",
    "                else:\n",
    "                    print(f\"Table '{table_name}' does not exist in schema '{schema_name}'.\")\n",
    "            print(\"----- End of Checking Tables -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Tables Column Names in the Raw Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_table_columns(connection_uri, schema_name, tables_in_schema):\n",
    "    \"\"\"\n",
    "    Fetches column names for a set of tables in a specified schema from a database.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The database connection URI.\n",
    "        schema_name (str): The schema name where the tables are located.\n",
    "        tables_in_silver (list of str): A list of table names for which the column names are to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are table names and the values are lists of column names for each table.\n",
    "    \"\"\"\n",
    "    columns_dict = {}\n",
    "    try:\n",
    "        engine = create_db_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            for table_name in tables_in_schema:\n",
    "                query = text(f\"\"\"\n",
    "                    SELECT column_name \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema = '{schema_name}' \n",
    "                    AND table_name = '{table_name}';\n",
    "                \"\"\")\n",
    "                result = connection.execute(query)\n",
    "                columns = [row[0] for row in result]  # Extract the first element (column_name) and create a list columns of column_names\n",
    "                columns_dict[table_name] = columns # Fill the columns_dict with keys (table_name) and values (list of column names) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching view columns: {str(e)}\")\n",
    "\n",
    "    return columns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Raw Table Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_table_data_types():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with data types for columns in raw tables.\n",
    "    \"\"\"\n",
    "    raw_data_types = {\n",
    "        'customers': {\n",
    "            'CustomerID': INT,\n",
    "            'Name': VARCHAR(100),\n",
    "            'Age': INT,\n",
    "            'Gender': VARCHAR(10),\n",
    "            'SignupDate': DATE\n",
    "        },\n",
    "        'dates': {\n",
    "            'DateID': INT,\n",
    "            'Date': DATE,\n",
    "            'Week': INT,\n",
    "            'Month': INT,\n",
    "            'Quarter': INT,\n",
    "            'Year': INT\n",
    "        },\n",
    "        'product_usage': {\n",
    "            'UsageID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'DateID': INT,\n",
    "            'ProductID': INT,\n",
    "            'NumLogins': INT,\n",
    "            'Amount': DECIMAL(10, 2)\n",
    "        },\n",
    "        'products': {\n",
    "            'ProductID': INT,\n",
    "            'ProductName': VARCHAR(100),\n",
    "            'Category': VARCHAR(50),\n",
    "            'Price': DECIMAL(10, 2)\n",
    "        },\n",
    "        'subscriptions': {\n",
    "            'SubscriptionID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'StartDate': DATE,\n",
    "            'EndDate': DATE,\n",
    "            'Type': VARCHAR(50),\n",
    "            'Status': VARCHAR(50)\n",
    "        },\n",
    "        'support_interactions': {\n",
    "            'InteractionID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'DateID': INT,\n",
    "            'IssueType': VARCHAR(100),\n",
    "            'ResolutionTime': INT\n",
    "        }\n",
    "    }\n",
    "    return raw_data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract data from CSV files\n",
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    # Test if a folder path exists\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a list of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary where keys are table names and values are DataFrames containing data from each CSV file\n",
    "    # This allows us to iterate over all the tables and perform specific transformations in the transform_raw() function  \n",
    "    data_frames = {}\n",
    "\n",
    "    # Iterating over each CSV file in the folder\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"-> CSV file '{csv_file}' loaded successfully.\")\n",
    "            \n",
    "            # Add 'extracted_at' column with current timestamp\n",
    "            # df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion into RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_raw(dfs, connection_uri, raw_schema_name):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into a RAW PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are columns to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are DataFrames with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----- Ingesting Data Into Raw. -----\")\n",
    "\n",
    "    try:\n",
    "        # Create the database engine\n",
    "        engine = create_engine(connection_uri)\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Set the search path to the specified schema\n",
    "            set_search_path_query = text(f\"SET search_path TO {raw_schema_name};\")\n",
    "            connection.execute(set_search_path_query)\n",
    "            print(f\"Search path set to schema '{raw_schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            print(\"-- to_sql() Ingestion Procedure in Raw. --\")\n",
    "            for table_name, cleaned_data_df in dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Ingesting data into {raw_schema_name}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp columns\n",
    "                # cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Get data types for the table from the dictionary\n",
    "                raw_data_types = get_raw_table_data_types()\n",
    "                data_type_dict = raw_data_types.get(table_name)\n",
    "\n",
    "                if data_type_dict is None:\n",
    "                    raise ValueError(f\"Data types not found for table '{table_name}' in Raw.\")\n",
    "\n",
    "                # Ingest data into the specified schema and table with specified data types\n",
    "                cleaned_data_df.to_sql(table_name, engine, schema=raw_schema_name, if_exists='replace', index=False, dtype=data_type_dict)\n",
    "\n",
    "                print(f\"-> CSV data ingested successfully into {raw_schema_name}.{table_name}.\")\n",
    "\n",
    "        print(\"----- END OF DATA INGESTION INTO RAW -----\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestion Parameters for RAW\n",
    "csv_folder_path = '/workspace/external_postgres/data'\n",
    "schema_names = ['raw']\n",
    "raw_schema = 'raw'\n",
    "\n",
    "create_schemas_script_path = 'create_schemas.sql'\n",
    "\n",
    "# Note, do not alter the order of the table names in silver, or it will not be ingested correctly\n",
    "tables_in_raw = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "\n",
    "# Define date columns for each table in Raw to perform specific transformations to conform to PostgreSQL syntax.\n",
    "date_columns_map = {            \n",
    "    # Date columns should be follow the CSV files column names\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "\n",
    "# Execute functions (please, respect the order)\n",
    "    # 1) Run create_schemas.sql\n",
    "    # 2) (Raw) Calling the Extract Function for all CSV files\n",
    "    # 3) (Raw) Data Ingestion with Minor Transformation\n",
    "\n",
    "# 1) Run create_schemas.sql \n",
    "print(\"----- Creating SCHEMAS in PostgreSQL -----\")\n",
    "result = run_sql_script(create_schemas_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Schemas were created.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# 2) (Raw) Calling the Extract Function for all CSV files\n",
    "print(\" -- Extract Function. --\")\n",
    "raw_data_dfs = extract(csv_folder_path)\n",
    "if raw_data_dfs is None:\n",
    "    print(\"Extraction failed.\")\n",
    "\n",
    "# 3) (Raw) Data Ingestion with Minor Transformation\n",
    "ingest_csv_to_raw(raw_data_dfs, connection_uri, raw_schema)\n",
    "\n",
    "# TEST COLUMN NAMES BEFORE INGESTION IN RAW\n",
    "print(\"--- PRINTING COLUMN NAMES AFTER INGESTING IN RAW---\")\n",
    "raw_column_names_before_ingestion = get_schema_table_columns(connection_uri, raw_schema, tables_in_raw)\n",
    "print(raw_column_names_before_ingestion.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
