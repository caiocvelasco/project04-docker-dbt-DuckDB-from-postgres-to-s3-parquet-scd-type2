{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import os\n",
    "from subprocess import call\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, INT, VARCHAR, DATE, TIMESTAMP, DECIMAL, BOOLEAN\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Schemas, Tables, and Views in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing SQL Scripts Against PostgreSQL (Schemas, Tables, Views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run SQL script using shell command\n",
    "# I had to pass the env parameters explicitly  to the subprocess.call() -> (PGPASSWORD, PGUSER, PGHOST, PGPORT, PGDATABASE)\n",
    "# This avoided Jupyter Notebook asking for password. \n",
    "def run_sql_script(script_name):\n",
    "    script_path = f\"/workspace/postgres_in_prod_example/{script_name}\"\n",
    "    command = f\"psql -U {user} -d {db_name} -h {host} -p {port} -f {script_path}\"\n",
    "    return call(command, shell=True, env={\n",
    "                                        'PGPASSWORD': password,\n",
    "                                        'PGUSER': user,\n",
    "                                        'PGHOST': host,\n",
    "                                        'PGPORT': port,\n",
    "                                        'PGDATABASE': db_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Schemas Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check schema existence\n",
    "def check_schema_existence(connection_uri, schema_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Schemas exist in the database ---\")\n",
    "            for schema_name in schema_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if schema_exists:\n",
    "                    print(f\"Schema '{schema_name}' exists in the database.\")\n",
    "                else:\n",
    "                    print(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(\"----- End of Schema Checking -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Tables Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check table existence\n",
    "def check_table_existence(connection_uri, schema_name, table_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Tables exist ---\")\n",
    "            for table_name in table_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = :schema AND table_name = :table\"),\n",
    "                    {\"schema\": schema_name, \"table\": table_name}\n",
    "                )\n",
    "                table_exists = result.fetchone() is not None\n",
    "                if table_exists:\n",
    "                    print(f\"Table '{table_name}' exists in schema '{schema_name}'.\")\n",
    "                else:\n",
    "                    print(f\"Table '{table_name}' does not exist in schema '{schema_name}'.\")\n",
    "            print(\"----- End of Checking Tables -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Tables Column Names in the Raw Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_table_columns(connection_uri, schema_name, tables_in_schema):\n",
    "    \"\"\"\n",
    "    Fetches column names for a set of tables in a specified schema from a database.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The database connection URI.\n",
    "        schema_name (str): The schema name where the tables are located.\n",
    "        tables_in_silver (list of str): A list of table names for which the column names are to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are table names and the values are lists of column names for each table.\n",
    "    \"\"\"\n",
    "    columns_dict = {}\n",
    "    try:\n",
    "        engine = create_db_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            for table_name in tables_in_schema:\n",
    "                query = text(f\"\"\"\n",
    "                    SELECT column_name \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema = '{schema_name}' \n",
    "                    AND table_name = '{table_name}';\n",
    "                \"\"\")\n",
    "                result = connection.execute(query)\n",
    "                columns = [row[0] for row in result]  # Extract the first element (column_name) and create a list columns of column_names\n",
    "                columns_dict[table_name] = columns # Fill the columns_dict with keys (table_name) and values (list of column names) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching view columns: {str(e)}\")\n",
    "\n",
    "    return columns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Raw Table Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_table_data_types():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with data types for columns in raw tables.\n",
    "    \"\"\"\n",
    "    raw_data_types = {\n",
    "        'customers': {\n",
    "            'CustomerID': INT,\n",
    "            'Name': VARCHAR(100),\n",
    "            'Age': INT,\n",
    "            'Gender': VARCHAR(10),\n",
    "            'SignupDate': DATE\n",
    "        },\n",
    "        'dates': {\n",
    "            'DateID': INT,\n",
    "            'Date': DATE,\n",
    "            'Week': INT,\n",
    "            'Month': INT,\n",
    "            'Quarter': INT,\n",
    "            'Year': INT\n",
    "        },\n",
    "        'product_usage': {\n",
    "            'UsageID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'DateID': INT,\n",
    "            'ProductID': INT,\n",
    "            'NumLogins': INT,\n",
    "            'Amount': DECIMAL(10, 2)\n",
    "        },\n",
    "        'products': {\n",
    "            'ProductID': INT,\n",
    "            'ProductName': VARCHAR(100),\n",
    "            'Category': VARCHAR(50),\n",
    "            'Price': DECIMAL(10, 2)\n",
    "        },\n",
    "        'subscriptions': {\n",
    "            'SubscriptionID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'StartDate': DATE,\n",
    "            'EndDate': DATE,\n",
    "            'Type': VARCHAR(50),\n",
    "            'Status': VARCHAR(50)\n",
    "        },\n",
    "        'support_interactions': {\n",
    "            'InteractionID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'DateID': INT,\n",
    "            'IssueType': VARCHAR(100),\n",
    "            'ResolutionTime': INT\n",
    "        }\n",
    "    }\n",
    "    return raw_data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract (from CSV to Pandas Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract data from CSV files\n",
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    # Test if a folder path exists\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a list of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary where keys are table names and values are DataFrames containing data from each CSV file\n",
    "    # This allows us to iterate over all the tables and perform specific transformations in the transform_raw() function  \n",
    "    data_frames = {}\n",
    "\n",
    "    # Iterating over each CSV file in the folder\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"-> CSV file '{csv_file}' loaded successfully.\")\n",
    "            \n",
    "            # Add 'extracted_at' column with current timestamp\n",
    "            # df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion (From Pandas Dataframe to Postgres RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_raw(dfs, connection_uri, raw_schema_name):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into a RAW PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are columns to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are DataFrames with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----- Ingesting Data Into Raw. -----\")\n",
    "\n",
    "    try:\n",
    "        # Create the database engine\n",
    "        engine = create_engine(connection_uri)\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Set the search path to the specified schema\n",
    "            set_search_path_query = text(f\"SET search_path TO {raw_schema_name};\")\n",
    "            connection.execute(set_search_path_query)\n",
    "            print(f\"Search path set to schema '{raw_schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            print(\"-- to_sql() Ingestion Procedure in Raw. --\")\n",
    "            for table_name, cleaned_data_df in dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Ingesting data into {raw_schema_name}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp columns\n",
    "                # cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Get data types for the table from the dictionary\n",
    "                raw_data_types = get_raw_table_data_types()\n",
    "                data_type_dict = raw_data_types.get(table_name)\n",
    "\n",
    "                if data_type_dict is None:\n",
    "                    raise ValueError(f\"Data types not found for table '{table_name}' in Raw.\")\n",
    "\n",
    "                # Ingest data into the specified schema and table with specified data types\n",
    "                cleaned_data_df.to_sql(table_name, engine, schema=raw_schema_name, if_exists='replace', index=False, dtype=data_type_dict)\n",
    "\n",
    "                print(f\"-> CSV data ingested successfully into {raw_schema_name}.{table_name}.\")\n",
    "\n",
    "        print(\"----- END OF DATA INGESTION INTO RAW -----\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing SCD Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting/Updating on CUSTOMERS_SCD_T2\n",
    "\n",
    "**initial_load_query**\n",
    "* The `last_updated` timestamp is used to determine whether a record has changed since the last update. This helps identify which records need to be updated or inserted in the SCD Type 2 table.\n",
    "* `start_date` and `end_date` are used to manage the historical data.\n",
    "* `start_date`: it is set to CURRENT_TIMESTAMP in the initial load, so it reflects the time at which the data was loaded into the SCD table.\n",
    "* `end_date`: \n",
    "    * it is set to NULL in the initial load, because, at the time of the load, these records are the most current and do not have an end date yet.\n",
    "    * The end_date will be updated later when a new version of the record is inserted (i.e., when the record becomes historical).\n",
    "\n",
    "**scd_update_query**\n",
    "\n",
    "If CUSTOMERS_SCD_T2 is not empty:\n",
    "* `latest_customers` (\"source table\")\n",
    "    * Creates a temporary result set with the current state of the records from the CUSTOMERS table.\n",
    "    * It fetches the latest data from the customers table with current timestamps and marks them as current.\n",
    "* `updated_records`\n",
    "    * Inserts new or updates existing records in the CUSTOMERS_SCD_T2 table that need to be marked as historical because their values have changed. \n",
    "    * A record is considered to have changed if any of its relevant fields differ between the source table (`latest_customers`), which is basically the CUSTOMERS table, and the target SCD Type 2 table (CUSTOMERS_SCD_T2).\n",
    "    * The logic is:\n",
    "        * To update the `end_date` to the current timestamp and to set `is_current` to FALSE for records where:\n",
    "            * The `customer_id` matches between CUSTOMERS_SCD_T2 and `latest_customers`.\n",
    "            * The existing record is currently marked as valid (is_current = TRUE).\n",
    "            * There is a difference in any of the relevant fields (name, age, gender, signup_date, last_updated).\n",
    "        * Returns: The `customer_id` of the updated records, which will be used to filter the records to insert.\n",
    "* `new_records`\n",
    "    * Inserts new or updated records into CUSTOMERS_SCD_T2 if they are not already present or have different values compared to the current records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will update CUSTOMER_SCD_T2 based on changes in the CUSTOMERS table.\n",
    "def update_customers_scd_t2(connection_uri, raw_schema_name):\n",
    "    print(\"----- Updating CUSTOMERS_SCD_T2 Table -----\")\n",
    "\n",
    "    try:\n",
    "        engine = create_engine(connection_uri)\n",
    "\n",
    "        with engine.connect() as connection:\n",
    "            # Begin a transaction\n",
    "            trans = connection.begin()\n",
    "\n",
    "            try:\n",
    "                # Set the search path to ensure we're working with the correct schema\n",
    "                set_search_path_query = text(f\"SET search_path TO {raw_schema_name};\")\n",
    "                connection.execute(set_search_path_query)\n",
    "                print(f\"Search path set to schema '{raw_schema_name}'.\")\n",
    "\n",
    "                # Check if the CUSTOMERS_SCD_T2 table is empty\n",
    "                check_empty_query = text(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM {raw_schema_name}.customers_scd_t2\n",
    "                \"\"\")\n",
    "                \n",
    "                result = connection.execute(check_empty_query)\n",
    "                count = result.scalar()  # Get the count of rows\n",
    "\n",
    "                if count == 0:\n",
    "                    # Table is empty, perform initial load\n",
    "                    print(\"-> Table is empty. Performing initial load.\")\n",
    "                    \n",
    "                    initial_load_query = text(f\"\"\"\n",
    "                        INSERT INTO {raw_schema_name}.customers_scd_t2 (\n",
    "                            customer_id, name, age, gender, signup_date, last_updated, start_date, end_date, is_current\n",
    "                        )\n",
    "                        SELECT\n",
    "                            \"CustomerID\" AS customer_id,\n",
    "                            \"Name\" AS name,\n",
    "                            \"Age\" AS age,\n",
    "                            \"Gender\" AS gender,\n",
    "                            \"SignupDate\" AS signup_date,\n",
    "                            \"last_updated\"::TIMESTAMP AS last_updated,\n",
    "                            CURRENT_TIMESTAMP AS start_date,\n",
    "                            NULL::TIMESTAMP AS end_date,\n",
    "                            TRUE AS is_current\n",
    "                        FROM {raw_schema_name}.customers\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    connection.execute(initial_load_query)\n",
    "                    print(\"-> Initial load completed successfully.\")\n",
    "                else:\n",
    "                    # Table is not empty, perform updates\n",
    "                    print(\"-> Table is not empty. Performing SCD Type 2 update.\")\n",
    "                    \n",
    "                    scd_update_query = text(f\"\"\"\n",
    "                        WITH latest_customers AS (\n",
    "                            SELECT\n",
    "                                \"CustomerID\" AS customer_id,\n",
    "                                \"Name\" AS name,\n",
    "                                \"Age\" AS age,\n",
    "                                \"Gender\" AS gender,\n",
    "                                \"SignupDate\" AS signup_date,\n",
    "                                \"last_updated\"::TIMESTAMP AS last_updated,\n",
    "                                CURRENT_TIMESTAMP AS start_date,\n",
    "                                NULL::TIMESTAMP AS end_date,\n",
    "                                TRUE AS is_current\n",
    "                            FROM {raw_schema_name}.customers\n",
    "                        ),\n",
    "                        -- Mark the old records as historical\n",
    "                        updated_records AS (\n",
    "                            UPDATE {raw_schema_name}.customers_scd_t2\n",
    "                            SET end_date = CURRENT_TIMESTAMP,\n",
    "                                is_current = FALSE\n",
    "                            FROM latest_customers lc\n",
    "                            WHERE {raw_schema_name}.customers_scd_t2.customer_id = lc.customer_id\n",
    "                                AND {raw_schema_name}.customers_scd_t2.is_current = TRUE\n",
    "                                AND (\n",
    "                                    {raw_schema_name}.customers_scd_t2.name <> lc.name OR\n",
    "                                    {raw_schema_name}.customers_scd_t2.age <> lc.age OR\n",
    "                                    {raw_schema_name}.customers_scd_t2.gender <> lc.gender OR\n",
    "                                    {raw_schema_name}.customers_scd_t2.signup_date <> lc.signup_date OR\n",
    "                                    {raw_schema_name}.customers_scd_t2.last_updated <> lc.last_updated\n",
    "                                )\n",
    "                            RETURNING {raw_schema_name}.customers_scd_t2.customer_id\n",
    "                        )\n",
    "                        -- Insert new records only if they are not present or updated\n",
    "                        INSERT INTO {raw_schema_name}.customers_scd_t2 (\n",
    "                                customer_id, name, age, gender, signup_date, last_updated, start_date, end_date, is_current\n",
    "                            )\n",
    "                            SELECT\n",
    "                                lc.customer_id,\n",
    "                                lc.name,\n",
    "                                lc.age,\n",
    "                                lc.gender,\n",
    "                                lc.signup_date,\n",
    "                                lc.last_updated,\n",
    "                                lc.start_date,\n",
    "                                lc.end_date,\n",
    "                                lc.is_current\n",
    "                            FROM latest_customers lc\n",
    "                            LEFT JOIN {raw_schema_name}.customers_scd_t2 scd\n",
    "                                ON lc.customer_id = scd.customer_id AND scd.is_current = TRUE\n",
    "                            WHERE scd.customer_id IS NULL\n",
    "                                OR (scd.name <> lc.name\n",
    "                                    OR scd.age <> lc.age\n",
    "                                    OR scd.gender <> lc.gender\n",
    "                                    OR scd.signup_date <> lc.signup_date\n",
    "                                    OR scd.last_updated <> lc.last_updated)\n",
    "                    \"\"\")\n",
    "\n",
    "                    connection.execute(scd_update_query)\n",
    "                    print(\"-> CUSTOMERS_SCD_T2 table updated successfully.\")\n",
    "                \n",
    "                # Commit the transaction\n",
    "                trans.commit()\n",
    "                print(\"-> Transaction committed successfully.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Rollback the transaction if any error occurs\n",
    "                trans.rollback()\n",
    "                print(f\"Transaction rolled back due to an error: {str(e)}\")\n",
    "                raise e\n",
    "\n",
    "        print(\"----- END OF SCD TYPE 2 UPDATE -----\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or updating SCD Type 2 data: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating CUSTOMER_SCD_T2 based on (manual) changes in the CUSTOMERS table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we manually update the CUSTOMERS table to check what will happen in the CUSTOMERS_SCD_T2\n",
    "def manually_update_customer(connection_uri, raw_schema_name):\n",
    "    print(\"----- MANUAL UPDATE ON CUSTOMERS Table -----\")\n",
    "    try:\n",
    "        engine = create_engine(connection_uri)\n",
    "\n",
    "        with engine.connect() as connection:\n",
    "            # Begin a transaction\n",
    "            trans = connection.begin()\n",
    "\n",
    "            try:\n",
    "                # Explicitly set the search path to ensure we're updating the correct schema\n",
    "                set_search_path_query = text(f\"SET search_path TO {raw_schema_name};\")\n",
    "                connection.execute(set_search_path_query)\n",
    "                print(f\"Search path set to schema '{raw_schema_name}'.\")\n",
    "\n",
    "                # Manually update a customer's record, including the last_updated column\n",
    "                update_query = f\"\"\"\n",
    "                    UPDATE {raw_schema_name}.customers\n",
    "                    SET \"Name\" = 'UPDATE_TEST',\n",
    "                        \"last_updated\" = CURRENT_TIMESTAMP\n",
    "                    WHERE \"CustomerID\" = 101;\n",
    "                \"\"\"\n",
    "\n",
    "                connection.execute(text(update_query))\n",
    "                print(\"Customer record updated successfully.\")\n",
    "\n",
    "                # Verify the update\n",
    "                verify_query = f\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM {raw_schema_name}.customers\n",
    "                    WHERE \"CustomerID\" = 101;\n",
    "                \"\"\"\n",
    "                result = connection.execute(text(verify_query))\n",
    "                updated_customer = result.fetchone()\n",
    "                print(f\"Updated Customer Record: {updated_customer}\")\n",
    "\n",
    "                # Commit the transaction\n",
    "                trans.commit()\n",
    "            except Exception as e:\n",
    "                # Rollback the transaction if any error occurs\n",
    "                trans.rollback()\n",
    "                raise e\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or updating customer data: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and Loading CSV files into Postgres RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Creating SCHEMAS in PostgreSQL -----\n",
      "CREATE SCHEMA\n",
      "CREATE TABLE\n",
      "SQL script executed successfully. Schemas were created.\n",
      "Database engine created successfully.\n",
      "--- Checking if Schemas exist in the database ---\n",
      "Schema 'raw' exists in the database.\n",
      "----- End of Schema Checking -----\n",
      " -- Extract Function. --\n",
      "-> CSV file 'customers.csv' loaded successfully.\n",
      "-> CSV file 'dates.csv' loaded successfully.\n",
      "-> CSV file 'products.csv' loaded successfully.\n",
      "-> CSV file 'product_usage.csv' loaded successfully.\n",
      "-> CSV file 'subscriptions.csv' loaded successfully.\n",
      "-> CSV file 'support_interactions.csv' loaded successfully.\n",
      "----- Ingesting Data Into Raw. -----\n",
      "Search path set to schema 'raw'.\n",
      "-- to_sql() Ingestion Procedure in Raw. --\n",
      "Ingesting data into raw.customers...\n",
      "-> CSV data ingested successfully into raw.customers.\n",
      "Ingesting data into raw.dates...\n",
      "-> CSV data ingested successfully into raw.dates.\n",
      "Ingesting data into raw.products...\n",
      "-> CSV data ingested successfully into raw.products.\n",
      "Ingesting data into raw.product_usage...\n",
      "-> CSV data ingested successfully into raw.product_usage.\n",
      "Ingesting data into raw.subscriptions...\n",
      "-> CSV data ingested successfully into raw.subscriptions.\n",
      "Ingesting data into raw.support_interactions...\n",
      "-> CSV data ingested successfully into raw.support_interactions.\n",
      "----- END OF DATA INGESTION INTO RAW -----\n",
      "--- PRINTING COLUMN NAMES AFTER INGESTING IN RAW---\n",
      "Database engine created successfully.\n",
      "dict_items([('customers', ['CustomerID', 'Name', 'Age', 'Gender', 'SignupDate', 'last_updated']), ('dates', ['DateID', 'Date', 'Week', 'Month', 'Quarter', 'Year']), ('product_usage', ['UsageID', 'CustomerID', 'DateID', 'ProductID', 'NumLogins', 'Amount']), ('products', ['ProductID', 'ProductName', 'Category', 'Price']), ('subscriptions', ['SubscriptionID', 'CustomerID', 'StartDate', 'EndDate', 'Type', 'Status']), ('support_interactions', ['InteractionID', 'CustomerID', 'DateID', 'IssueType', 'ResolutionTime'])])\n"
     ]
    }
   ],
   "source": [
    "# Ingestion Parameters for RAW\n",
    "csv_folder_path = '/workspace/postgres_in_prod_example/data'\n",
    "schema_names = ['raw']\n",
    "raw_schema = 'raw'\n",
    "\n",
    "create_schemas_script_path = 'create_schemas.sql'\n",
    "\n",
    "# Note, do not alter the order of the table names in silver, or it will not be ingested correctly\n",
    "tables_in_raw = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "\n",
    "# Define date columns for each table in Raw to perform specific transformations to conform to PostgreSQL syntax.\n",
    "date_columns_map = {            \n",
    "    # Date columns should be follow the CSV files column names\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "# Executing functions (please, respect the order)\n",
    "    # 1) Run create_schemas.sql\n",
    "    # 2) (Raw) Calling the Extract Function for all CSV files\n",
    "    # 3) (Raw) Data Ingestion with Minor Transformation\n",
    "\n",
    "# 1) Run create_schemas.sql \n",
    "print(\"----- Creating SCHEMAS in PostgreSQL -----\")\n",
    "result = run_sql_script(create_schemas_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Schemas were created.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# 2) (Raw) Calling the Extract Function for all CSV files\n",
    "print(\" -- Extract Function. --\")\n",
    "raw_data_dfs = extract(csv_folder_path)\n",
    "if raw_data_dfs is None:\n",
    "    print(\"Extraction failed.\")\n",
    "\n",
    "# 3) (Raw) Data Ingestion with Minor Transformation\n",
    "ingest_csv_to_raw(raw_data_dfs, connection_uri, raw_schema)\n",
    "\n",
    "# TEST COLUMN NAMES BEFORE INGESTION IN RAW\n",
    "print(\"--- PRINTING COLUMN NAMES AFTER INGESTING IN RAW---\")\n",
    "raw_column_names_before_ingestion = get_schema_table_columns(connection_uri, raw_schema, tables_in_raw)\n",
    "print(raw_column_names_before_ingestion.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting for First Load or Uploading for following loads into CUSTOMERS_SCD_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Updating CUSTOMERS_SCD_T2 Table -----\n",
      "Search path set to schema 'raw'.\n",
      "-> Table is empty. Performing initial load.\n",
      "-> Initial load completed successfully.\n",
      "-> Transaction committed successfully.\n",
      "----- END OF SCD TYPE 2 UPDATE -----\n"
     ]
    }
   ],
   "source": [
    "# First load of CUSTOMERS_SCD_T2 Table based on data in CUSTOMERS\n",
    "update_customers_scd_t2(connection_uri, raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- MANUAL UPDATE ON CUSTOMERS Table -----\n",
      "Search path set to schema 'raw'.\n",
      "Customer record updated successfully.\n",
      "Updated Customer Record: (101, 'UPDATE_TEST', 30, 'M', datetime.date(2020, 1, 15), '2024-07-31 11:50:59.556905+00')\n"
     ]
    }
   ],
   "source": [
    "# Update a customer record\n",
    "manually_update_customer(connection_uri, raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Updating CUSTOMERS_SCD_T2 Table -----\n",
      "Search path set to schema 'raw'.\n",
      "-> Table is not empty. Performing SCD Type 2 update.\n",
      "-> CUSTOMERS_SCD_T2 table updated successfully.\n",
      "-> Transaction committed successfully.\n",
      "----- END OF SCD TYPE 2 UPDATE -----\n"
     ]
    }
   ],
   "source": [
    "# Capture the changes in CUSTOMERS_SCD_T2 Table based on changes on CUSTOMERS\n",
    "update_customers_scd_t2(connection_uri, raw_schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
